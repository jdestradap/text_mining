{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<center> <h2>Punto 1 </h2> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paquetes\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords as SW\n",
    "from nltk.stem.porter import PorterStemmer as pst\n",
    "from nltk.stem.lancaster import LancasterStemmer as lst\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(['punkt', 'stopwords','wordnet'])\n",
    "sw = SW.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 =  boto3.client('s3', region_name='us-east-1')\n",
    "with open('news.csv', 'wb') as f:\n",
    "    s3.download_fileobj('finaltext','news.csv', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('news.csv')\n",
    "\n",
    "print('se tienen las siguientes ' + str(news.shape[1]) + ' columnas.\\n')\n",
    "print(news.columns.tolist())\n",
    "\n",
    "print('\\n')\n",
    "print('Se imprimen los primeros registros y encabezados\\n')\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de columnas\n",
    "procesamiento = news[[\"title\", \"content\"]]\n",
    "print(\"La columna 1 es: \" + procesamiento.columns[0])\n",
    "print(\"La columna 2 es: \" + procesamiento.columns[1])\n",
    "print('-----')\n",
    "\n",
    "procesamiento['content'] = procesamiento['content'].astype(str)\n",
    "procesamiento['title'] = procesamiento['title'].astype(str)\n",
    "\n",
    "# como análisis integrado\n",
    "procesamiento[\"integrado\"] = [x[0]+\" \"+x[1] for x in zip(procesamiento[\"title\"], procesamiento[\"content\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Universo = ' '.join(procesamiento[\"integrado\"])\n",
    "Tokens = nltk.word_tokenize(Universo)\n",
    "\n",
    "# Limpieza de caracteres especiales\n",
    "Tokens = [re.sub('[^A-Za-z0-9]+', '', w) for w in Tokens]\n",
    "Tokens = [w.lower() for w in Tokens]\n",
    "Tokens = [w for w in Tokens if len(w)>1]\n",
    "\n",
    "unicos = list(dict.fromkeys(nltk.FreqDist(Tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para ver evolucion del numero de tokens\n",
    "n_Tok_nltk = []\n",
    "n_Tok_Uniq_nltk = []\n",
    "proc_nltk =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para ver como evoluciona el número de tokens\n",
    "n_Tok_nltk.append(len(Tokens))\n",
    "n_Tok_Uniq_nltk.append(len(unicos))\n",
    "proc_nltk.append('Tokenización')\n",
    "print('Haciendo Tokenización con NLTK, teniendo en cuenta solo caracteres alfabéticos, se tienen ' + \n",
    "      str(len(Tokens)) + \n",
    "      ' Tokens con ' + \n",
    "      str(len(unicos)) + \n",
    "      ' valores únicos.')\n",
    "print('---')\n",
    "\n",
    "# Quitar Stopwords\n",
    "Tokens = [w for w in Tokens if w not in sw]\n",
    "\n",
    "unicos = list(dict.fromkeys(nltk.FreqDist(Tokens)))\n",
    "\n",
    "# Para ver como evoluciona el número de tokens\n",
    "n_Tok_nltk.append(len(Tokens))\n",
    "n_Tok_Uniq_nltk.append(len(unicos))\n",
    "proc_nltk.append('Remover Stopwords')\n",
    "print('Quitando stopwords se tienen ' + \n",
    "      str(len(Tokens)) + \n",
    "      ' Tokens con ' + \n",
    "      str(len(unicos)) + \n",
    "      ' valores únicos.')\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency\n",
    "Freq = nltk.FreqDist(Tokens)\n",
    "\n",
    "# Top 20 mas comunes\n",
    "M_Freq = Freq.most_common(20)\n",
    "print('Las palabras más frecuentes son: ')\n",
    "\n",
    "# 8.4\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(M_Freq)), [val[1] for val in M_Freq], align='center')\n",
    "plt.xticks(range(len(M_Freq)), [val[0] for val in M_Freq])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming de porter\n",
    "\n",
    "PorStem = pst()\n",
    "Tokens2 = [PorStem.stem(w) for w in Tokens]\n",
    "\n",
    "unicos = list(dict.fromkeys(nltk.FreqDist(Tokens2)))\n",
    "\n",
    "n_Tok_nltk.append(len(Tokens2))\n",
    "n_Tok_Uniq_nltk.append(len(unicos))\n",
    "proc_nltk.append('Porter Stemming')\n",
    "print('Aplicando el Stemming de Porter, se tienen ' +\n",
    "      str(len(Tokens2)) + \n",
    "      ' Tokens con ' + \n",
    "      str(len(unicos)) + \n",
    "      ' valores únicos.')\n",
    "print('---')\n",
    "\n",
    "# Term Freq porter\n",
    "Freq = nltk.FreqDist(Tokens2)\n",
    "M_Freq = Freq.most_common(20)\n",
    "print('Las palabras más frecuentes son: ')\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(M_Freq)), [val[1] for val in M_Freq], align='center')\n",
    "plt.xticks(range(len(M_Freq)), [val[0] for val in M_Freq])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming Lancaster\n",
    "\n",
    "LancStem = lst() # Lancaster Stemmer\n",
    "Tokens3 = [LancStem.stem(w) for w in Tokens]\n",
    "\n",
    "unicos = list(dict.fromkeys(nltk.FreqDist(Tokens3)))\n",
    "\n",
    "\n",
    "n_Tok_nltk.append(len(Tokens3))\n",
    "n_Tok_Uniq_nltk.append(len(unicos))\n",
    "proc_nltk.append('Lancaster Stemming')\n",
    "print('Aplicando el Stemming de Lancaster, se tienen ' +\n",
    "      str(len(Tokens3)) + \n",
    "      ' Tokens con ' + \n",
    "      str(len(unicos)) + \n",
    "      ' valores únicos.')\n",
    "print('---')\n",
    "\n",
    "# Term Freq Lancaster\n",
    "Freq = nltk.FreqDist(Tokens3)\n",
    "M_Freq = Freq.most_common(20)\n",
    "print('Las palabras más frecuentes son: ')\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(M_Freq)), [val[1] for val in M_Freq], align='center')\n",
    "plt.xticks(range(len(M_Freq)), [val[0] for val in M_Freq])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "Tokens4 = [lemma.lemmatize(w, pos = 'v') for w in Tokens]\n",
    "\n",
    "unicos = list(dict.fromkeys(nltk.FreqDist(Tokens4)))\n",
    "\n",
    "\n",
    "n_Tok_nltk.append(len(Tokens4))\n",
    "n_Tok_Uniq_nltk.append(len(unicos))\n",
    "proc_nltk.append('Lemmatization')\n",
    "print('Aplicando Lematizacion de NLTK, se tienen ' +\n",
    "      str(len(Tokens4)) + \n",
    "      ' Tokens con ' + \n",
    "      str(len(unicos)) + \n",
    "      ' valores únicos.')\n",
    "print('---')\n",
    "\n",
    "# term freq lemma\n",
    "Freq = nltk.FreqDist(Tokens4)\n",
    "M_Freq = Freq.most_common(20)\n",
    "print('Las palabras más frecuentes son: ')\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(M_Freq)), [val[1] for val in M_Freq], align='center')\n",
    "plt.xticks(range(len(M_Freq)), [val[0] for val in M_Freq])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
